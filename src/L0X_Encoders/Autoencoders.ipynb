{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Автоенкодеры (AE). Вариационные автоэнкодеры (VAE). Условные вариационные автоенкодеры (CVAE). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Unsupervised learning \n",
    "\n",
    "<p style=\"font-family: Arial; font-size:1em;color:red;\"> TODO </p>\n",
    "\n",
    "Формальная постановка задачи\n",
    "\n",
    "Примеры  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation learning\n",
    "\n",
    "<p style=\"font-family: Arial; font-size:1em;color:red;\"> TODO </p>\n",
    "\n",
    "### Зачем нужно\n",
    "\n",
    "### \"Философское обоснование\"\n",
    "\n",
    "### Примеры успешных работ (взять из NIPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X68mLzjZpaiV"
   },
   "source": [
    "## Структура и обучение автоэнкодера.\n",
    "Autoencoder - архитектура нейросети, которая сначала с помощью нейросети-энкодера сжимает изображение в вектор небольшой размерности(он называется скрытым представлением), а затем восстанавливает этот вектор в исходную картинку с помощью нейросети-декодера. Казалось бы, зачем это нужно? Вход и выход этой нейросети - одна и та-же картинка, ничего нового она не создаёт. \n",
    "\n",
    "\n",
    "<p style=\"font-family: Arial; font-size:1em;color:red;\"> TODO - переписать этот абзац</p>\n",
    "Однако практика показывает, что скрытое представление картинки позволяет делать очень интересные и красивые вещи - например, гладкую интерполяцию между 2 написанными от руки цифрами. Более того, Autoencoder можно даже обучить очищать изображения от шума.\n",
    "\n",
    "Откуда берутся эти свойства? Они являются следствие сжатия информации. Одна из форм сжатия это классификация, которую мы уже делали. Если это цифры, то вместо изображения можно сохранить только какая это цифра. Это предельное сжатие информации, но при попытке перевести цифру в картинку мы уже не имеем достаточно информации, чтобы картинка получалось разной. Если не так сильно ограничивать информацию в точке максимального сжатия, то кроме класса цифры сохранится еще что-то и изображение удастся восстановить с большим количеством сохранённых деталей.\n",
    "\n",
    "\n",
    "![alt text](img/better_enc_dec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сжатие информации и потери\n",
    "Автоэнкодер может быть без потерь и с потерями (lossless и lossy). В какой-то степени это альтернативно методам сжатия архиваторов и кодирования контента (zip, mp3, jpeg, flac, ...). Можно ли сделать сжатие на нейронных сетях с помощью автоэнкодеров? Да, это будет работать. Размер сети будет большим, но сжатие может превзойти другие алгоритмы. Исследования в этой области ведутся, но практически применямых примеров я не знаю.\n",
    "![alt text](img/lossy_encoding.png)\n",
    "\n",
    "Почему это может работь? Дело в том, что нейронная сеть может сформулировать набор правил, по которому на основе латентного представления приближенно или точно восстанавливать исходный ообъект\n",
    "\n",
    "![alt text](img/bad_rule_img.png)\n",
    "\n",
    "А почему мы уверены, что такой набор правил будет существовать и мы вообще имеем право понижать размерность пространства?\n",
    "\n",
    "\n",
    "## Manifold assumption \n",
    "\n",
    "Мы предполагаем, что наши данные на самом деле лежат в простанстве меньшем, чем пространство исходных признаков\n",
    "\n",
    "![alt text](img/manifold1.png)\n",
    "\n",
    "В большинстве случаев это действительно правда. Например, лица людей даже на фотографиях 300x300, очевидно, лежат в пространстве меньшей размерности, нежели 90000. Ведь не каждая матрица 300 на 300, заполненная какими-то значениями от 0 до 1, даст нам изображение человека\n",
    "\n",
    "![alt text](img/manifold2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод главных компонент (PCA). Аналогия AE и PCA\n",
    "Метод главных компонент (Principal Component Analysis). Это метод отображения векторов свойств объектов (помним, что у нас объект всегда описывается вектором свойств, длина вектора это количество свойств) в вектора производных свойств (**компонент**), меньшей длины с помощью линейной комбинации, чтобы обратной операцией можно было восстановить значения векторов свойств как можно ближе к исходным. То есть PCA тоже выполняет сжатие информации, он тоже работает для группы объектов (а нейронная сеть автоэнкодера учится под определённую группу объектов). \n",
    "\n",
    "Обычно PCA работает для с центрированных переменных. Каждая следующая компонента проводится перпендикулярно предыдущим и так, чтобы объяснить наибольшую часть разницы, не объясненной предыдущими компонентами между объектами.\n",
    "\n",
    "![alt_text](img/pca_motivation.png)\n",
    "\n",
    "Графически можно представить PCA, как поиск подпространства, проекция точек на которое минимально меняет координаты в исходном пространстве. Например для объектов на плоскости PCA можно сделать в одномерное пространство - на прямую.\n",
    "![alt text](img/pca_decomposition.png)\n",
    "Прямая определяется только вектором нормали, то есть линия проекции проходит через 0. \n",
    "\n",
    "\n",
    "\n",
    "Отличие PCA и AE в том, что PCA выполняет линейную комбинацию над компонентами исходного вектора свойств объекта, а AE - как правило, нелинейную. PCA вычисляется однозначно, а AE обучается без гарантии нахождения наилучшего положения. PCA гарантирует ортогональный базис для разложения сжатых свойств, а AE - нет.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA будет частным случаем AE если в нём сделать только один плотный слой (Dense) с количеством нейронов, равным размерности сжатого состояния, сделать линейную функцию активации, сделать потери по среднему квадрату ошибки (mean squared error - MSE). Тогда PCA позволяет рассчитать веса для нейронов такого автоэнкодера. При этом гарантировав наилучшее решение задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Очищение изображения от шумов\n",
    "Интересное применение Autoencoder'ов - очищение входной картинки от шумов. Такое принципиально возможно из-за того, что размерность латентного пространства очень мала по сравнению с размерностью входного пространства(в нашем случае - 32 и 784 соответственно) - в нём попросту нет места случайному шуму, но зато есть место для общих закономерностей из входного пространства.\n",
    "То есть мы подаём на обучении автоэнкодера такой незашумлённый датасет, что в нём на самом деле есть некое простраство свойств, которое его описывает. На выходе энкодера в изображении останутся именно эти свойства. Шум является внешним свойством и не сможет закодироваться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте применим PCA, как простеший автоэнкодер, для очищения от шумов изображений базы MNIST. Нам потребуется база MNIST, numpy, библиотека отрисовки matplotlib и сам PCA, который есть в пакете sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные и приведём размерность к двумерной, чтобы это был набор векторов свойств."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train/255., x_test/255.\n",
    "xt_shape = x_train.shape\n",
    "print(\"Initial shape \", xt_shape)\n",
    "xt_flat = x_train.reshape(-1, xt_shape[1]*xt_shape[2])\n",
    "print(\"Reshaped to \", xt_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь заведём класс PCA и настроим его, чтобы сохранял 95% исходной картинки. Обучим его и посмотрим сколько ему потребовалось свойств для описания каждой картинки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(.90)\n",
    "xt_encoded = pca.fit_transform(xt_flat)\n",
    "print(\"Encoded features \", pca.n_components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Энкодер (он же декодер, ведь это просто обратная матрица от энкодера PCA) обучен. Теперь можно проверить, как он закодирует и раскодирует тестовую выборку. Для этого проведём такие же преобразования размерности для неё."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_shape = x_test.shape\n",
    "xtest_flat = x_test.reshape(-1, xtest_shape[1]*xtest_shape[2])\n",
    "xtest_encoded = pca.transform(xtest_flat)\n",
    "xtest_decoded = pca.inverse_transform(xtest_encoded).reshape(xtest_shape)\n",
    "print(\"Decoded xtest_decoded shape is \", xtest_decoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нужно определить функцию для отрисовки изображений MNIST. Она будет выводить несколько изображений в ряд, поэтому будет принимать трёхмерный массив. Шкала не должна быть автоподстраиваемой, так как после обработки изображения выйдут за диапазон (0,1), в котором заданы исходные изображения. Мы зафиксируем шкалу в диапазоне (0,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, title):\n",
    "    fig=plt.figure(figsize=(16, 3))\n",
    "    columns = images.shape[0]\n",
    "    rows = 1\n",
    "    for i in range(columns):\n",
    "        fig.add_subplot(rows, columns, i+1)\n",
    "        plt.imshow(images[i], cmap='gray_r', clim=(0,1))\n",
    "    fig.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем исходное и закодированное изображение для некоторых изображений, которые мы элегантно случайным образом выберем из всей тестовой выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_indices = np.random.choice(x_test.shape[0], 6)\n",
    "samples_orig = x_test[sample_indices]\n",
    "samples_decoded = xtest_decoded[sample_indices]\n",
    "plot_images(samples_orig, \"Original x_test\")\n",
    "plot_images(samples_decoded, \"PCA decoded x_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что `pca.n_components_` (87 для 90% PCA) достаточно для описания картинок MNIST вместо 784 исходных пикселей. Но при этом нужно хранить матрицу кодирования-декодирования, а также изображения получаются немного зашумлёнными. Мы получили способ сжатия с потерями для рукописных цифр, где изображение центрировано и отмасштабировано по рамке из 28х28 пикселей (подробней смотри правила базы MNIST). Степень сжатия у нас условно 87/784 ~= 0.11. То есть сжатие в 9 раз. \"Условно\", так как сжатое изображение хранится во float, а исходное в uint8, который требует в 4 раза меньше байт. Можешь сам на досуге сравнить какое качество изображения будет при аналогичном сжатии JPEG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посмотрим как наш автоэнкодер без нейросетей справится с очисткой от зашумления. Для этого сделаем функцию добавления шумов к MNIST выборке и посмотрим результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_add_noise(noise_factor, dataset):\n",
    "    return dataset + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=dataset.shape) \n",
    "\n",
    "x_test_noisy = mnist_add_noise(0.3, x_test)\n",
    "samples_noisy = x_test_noisy[sample_indices]\n",
    "plot_images(samples_noisy, \"x_test with added noise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нужно провести ту же операцию PCA энкодера и декодера, что выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCArecode(dataset):\n",
    "    dataset_flat = dataset.reshape(-1, dataset.shape[1]*dataset.shape[2])\n",
    "    return pca.inverse_transform(pca.transform(dataset_flat)).reshape(dataset.shape)\n",
    "\n",
    "x_filtered = PCArecode(x_test_noisy)\n",
    "samples_filtered = x_filtered[sample_indices]\n",
    "plot_images(samples_filtered, \"Noise filtered x_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, шумы стали значительно меньше, но и артефакты вокруг цифры усилились. Это неудивительно, ведь мы сжимали информацию линейным образом на целых 87 компоненты. Это не позволяет ни глубокого обучения, ни достаточного сжатия. Повышение уровня сжатия приведёт к еще большему количеству артефактов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1em;color:red;\"> TODO </p>\n",
    "\n",
    "### Латентное представление для цифр после PCA \n",
    "\n",
    "Видим, что отличить цифры друг от друга практически невозможно "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение автоэнкодера\n",
    "Итак, вспомним, что в автоэнкодере одна сеть переводит пространство свойств в пространство меньшей размерности, а другая сеть восстанавливает исходное изображение. Вместо вычисления коэффициентов сети мы будем её обучать. Для обучения нужно определить функцию потерь. Мы возьмём среднеквадратичное расстояние (MSE). То есть мы требуем, чтобы значения пикселей исходного изображения и восстановленного отличались несильно.\n",
    "![alt_text](img/encoder_loss.png)\n",
    "Мы можем использовать любую сеть для энкодера и декодера: на плотных слоя или на свёрточных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим автоэнкодер и посмотрим на качество декодирования изображений. Начнём с импорта стандартного набора библиотек. Теперь нам потребуется tensorflow с его плотными, свёрточными и другими стандартными слоями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k2OI4u7vJmtT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Dense, Dropout, Input, LeakyReLU, Flatten, Reshape\n",
    "from tensorflow.keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фикс для GPU\n",
    "Для видеокарт с 4 ГБ памяти и меньше tensorflow оставляет недостаточный запас свободной видеопамяти, поэтому включим выделение памяти по требованию, вместо дефолтной процеду выделения сразу почти всей памяти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нужно задать архитектуру модели. Мы будем использовать последовательную модель и свёрточную  архитектуру. В конце должен быть вектор длины `latent_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = (x_train / 255.0).astype(np.float32)\n",
    "x_test = (x_test / 255.0).astype(np.float32)\n",
    "\n",
    "# Convolutional layers require 4th dimension\n",
    "x_train = np.expand_dims(x_train, axis=3)\n",
    "x_test = np.expand_dims(x_test, axis=3)\n",
    "\n",
    "AE_latent_size = 24\n",
    "\n",
    "# Create Encoder\n",
    "def getEncoder(latent_size):\n",
    "  model = Sequential([\n",
    "      Conv2D(32, (3, 3), padding='same', input_shape=(28, 28, 1)),\n",
    "      LeakyReLU(0.2),\n",
    "      Conv2D(32, (3, 3), padding='same'),\n",
    "      LeakyReLU(0.2),\n",
    "      MaxPooling2D((2, 2), padding='same'),\n",
    "      Conv2D(64, (3, 3), padding='same'),\n",
    "      LeakyReLU(0.2),\n",
    "      Conv2D(64, (3, 3), padding='same'),\n",
    "      LeakyReLU(0.2),\n",
    "      MaxPooling2D((2, 2), padding='same'),\n",
    "      Flatten(),\n",
    "      Dense(units=latent_size)\n",
    "  ])\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "  return model\n",
    "\n",
    "# Create Decoder\n",
    "def getDecoder(latent_size):\n",
    "  model = Sequential([\n",
    "      Dense(units=7*7*64, input_dim=latent_size),\n",
    "      Reshape((7, 7, 64), input_shape=(7*7*64,)),\n",
    "      UpSampling2D((2, 2)),\n",
    "      Conv2D(64, (3, 3), padding='same'),\n",
    "      LeakyReLU(0.2),\n",
    "      Conv2D(64, (3, 3), padding='same'),\n",
    "      LeakyReLU(0.2),\n",
    "      UpSampling2D((2, 2)),\n",
    "      Conv2D(32, (3, 3), padding='same'),\n",
    "      LeakyReLU(0.2),\n",
    "      Conv2D(1, (3, 3), activation='sigmoid', padding='same'),\n",
    "  ])\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создадим автоэнкодер функциональным способом и установим ему функцию потерь MSE и наш стандартный оптимизатор - Adam. Обучим его батчами по 64 элемента и 10 эпох хватит с запасом. Напомним, что обучаем мы его восстанавливать исходную картинку, поэтому как input, так и label будет равен x_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = getEncoder(AE_latent_size)\n",
    "decoder = getDecoder(AE_latent_size)\n",
    "\n",
    "autoencoder_input = Input(shape=(28, 28, 1)) # Set shape to (28, 28, 1) if autoencoder is convolutional, (784, 1) otherwise\n",
    "latent = encoder(autoencoder_input)\n",
    "autoencoder_output= decoder(latent)\n",
    "autoencoder = Model(inputs=autoencoder_input, outputs=autoencoder_output)\n",
    "autoencoder.compile(loss='mse', optimizer='adam')\n",
    "history = autoencoder.fit(x_train, x_train, batch_size=64, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всё готово и можно сделать процедуру, как с PCA по кодированию и декодированию. Мы использовали дополнительную размерность для свёрточных сетей, поэтому надо будет её убрать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_decoded = np.squeeze(autoencoder.predict(x_test[sample_indices]), axis = 3)\n",
    "plot_images(samples_orig, \"Original x_test\")\n",
    "plot_images(samples_decoded, \"Autoencoder decoded x_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество сжатия мы оценили визуально выше. Если обратить внимание, то исходные картинки даже почистились от мелких шумов и странностей изображения и больше стали похожи на непрерывные линии. Размерность латентного пространства `latent_size` значительно меньше исходного количества свойств (784), поэтому мы получили неплохое сжатие изображения. И для этого нам потребовалась модель decoder, в которой есть коэффициенты. Посмотрим размер моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.summary()\n",
    "encoder.summary()\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Латентное представление для цифр после PCA\n",
    "\n",
    "<p style=\"font-family: Arial; font-size:1em;color:red;\"> TODO </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разреженный автоенкодер\n",
    "\n",
    "<p style=\"font-family: Arial; font-size:1em;color:red;\"> TODO </p>\n",
    "\n",
    "\n",
    "\n",
    "### L2 и L1-регуляризации\n",
    "Напоминание:\n",
    "\n",
    "Квадрат от значения аргумента называют L2-регуляризацией или гребневой регрессией. У неё нет свойства отбирать одни признаки против других, так как по мере приближения к нулю параболическая зависимость начинает давать мало вклада в потери, а вклад в основном приходит от больших аргументов.\n",
    "\n",
    "$$L2 = \\beta \\sum_i w_i^2 $$\n",
    "\n",
    "Модуль от значения аргумента называют L1-регуляризацией или лассо-регрессией. Эта регуляризация обычно довольно агрессивная и в многомерном пространстве приводит к отбору свойств или признаков объекта, обращая малозначимые коэффициенты под регуляризацией в ноль. То есть если надо минимизировать сумму модулей, то лучше удержать какой-то параметр на величину 'a', но свести малозначимый параметр со значением 'a' до нуля (меньше потери по нему уже не сделать). Последний параметр перестанет действовать, но зато более значимый параметр при разумных значения коэффициента регуляризации не пострадает.\n",
    "\n",
    "$$L1 = \\alpha \\sum_i |w_i| $$\n",
    "\n",
    "<img src=\"img/losses.gif\" alt=\"alttext\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "Для нашей задачи потому подойдет L1-loss\n",
    "\n",
    "Введем штрафы на **активации** нейронов - таким образом, мы будем поощрять нейросеть использовать для кодирования каждого объекта как можно меньше нейронов - следовательно, выбрасывать незначимую информацию. \n",
    "\n",
    "### KL-divergence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Автоэнкодер, как генератор и его ограничения. Плавная интерполяция\n",
    "У нас уже была система с латентным пространством и возможностью строить по нему объекты - GAN. Значит в случае автоэнкодеров тоже можно подавать случайный вектор на декодер и получать новые объекты. До этого мы же только восстанавливали исходную картинку.\n",
    "![alt text](img/encoder_as_generator.png)\n",
    "Какое значение вектора выбрать? Мы же никак не управляли латентным пространством. Непонятно какие числа подставлять. Поэтому мы можем выбрать промежуточные значения между двумя представлением двух исходных изображений в латентном пространстве и получить плавную интерполяцию между изображениями. Постепенно свойства одного изображения будут исчезать, а появляться свойства другого."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "id": "0yNPENXvZCUV",
    "outputId": "a14a25a3-d466-4c2c-fe24-c33882fb0044"
   },
   "outputs": [],
   "source": [
    "def interpolation(vec1, vec2, N_inter):\n",
    "    intermediate_values = np.zeros((0, vec1.shape[0]))\n",
    "    for i in range(N_inter):\n",
    "        intermediate = (1-float(i / N_inter))*vec1 + float(i / N_inter)*vec2\n",
    "        intermediate_values = np.append(intermediate_values, intermediate.reshape(1, -1), axis=0)\n",
    "    intermediate_values = np.append(intermediate_values, vec2.reshape(1, -1), axis=0)\n",
    "    return intermediate_values\n",
    "\n",
    "N_inter = 8\n",
    "\n",
    "# Take pairs of random images from the test set\n",
    "encodings = encoder.predict(np.expand_dims(samples_orig, axis = 3))\n",
    "for i in range(len(sample_indices) - 1):\n",
    "    vectors = interpolation(encodings[i], encodings[i+1], N_inter)\n",
    "    images = np.squeeze(decoder.predict(vectors), axis = 3)\n",
    "    plot_images(images, \"Interpolation of %i into %i\"%(y_test[sample_indices[i]], y_test[sample_indices[i+1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-08LyQu0te_a"
   },
   "source": [
    "Если этот файл запущен не на Google Colab, с помощью этого кода можно создать видео, на котором процесс превращения одной цифры в другую будет виден ещё более наглядно. Для этого можно использовать уже известный нам OpenCV. Он умеет делать видеофайлы из массивов чисел."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RSua0vyadurR"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "N_inter = 90\n",
    "resize_coeff = 10\n",
    "\n",
    "size = (images.shape[2]*resize_coeff, images.shape[1]*resize_coeff)\n",
    "out = cv2.VideoWriter('output/interpolation.avi',cv2.VideoWriter_fourcc(*'DIVX'), 30, size, 0)\n",
    "for i in range(len(sample_indices) - 1):\n",
    "    vectors = interpolation(encodings[i], encodings[i+1], N_inter)\n",
    "    images = np.squeeze(decoder.predict(vectors), axis = 3)\n",
    "    for i in range(len(images)):\n",
    "        img = images[i] / np.max(images[i])\n",
    "        img = (cv2.resize(255*img.reshape(28, 28), size, cv2.INTER_NEAREST))\n",
    "        out.write(img.astype(np.uint8))\n",
    "    \n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так себе интерполяция вышла. Старое изображение затухает, а новое изображение появляется. Хочется, чтобы в промежуточных кадрах не было каки-то непонятных очертаний, а изображение было чем-то промежуточным по смыслу между стартовым и конечным изображением. Причина в том, что в латентном пространстве действительно возникли зоны, которые умеют декодироваться в хорошие изображения. Но никто не сказал, что между этими зонами должно быть что-то адекватное (что мы видели из представления).\n",
    "Представим это графически. Пусть наш очень умный, содержащий очень много коэффициентов, энкодер и декодер смог разложить все входные объекты на одной оси (размерность латентного пространства - 1). По сути он каждому входному изображению присвоил номер и по номеру может это изображение вспомнить. То есть автоэнкодер очень переобученный. Тогда если мы возьмём промежуточный номер (пытаемся интерполировать), то какое изображение мы собираемся получить?\n",
    "![https://miro.medium.com/max/1395/1*iSfaVxcGi_ELkKgAG0YRlQ@2x.png](https://miro.medium.com/max/1395/1*iSfaVxcGi_ELkKgAG0YRlQ@2x.png)\n",
    "\n",
    "Если мы хотим, чтобы декодированные промежуточные латентные состояния имели черты близких к ним объектов, то надо притянуть латентные координаты похожих объектов. Например вот так:\n",
    "![https://miro.medium.com/max/1395/1*83S0T8IEJyudR_I5rI9now@2x.png](https://miro.medium.com/max/1395/1*83S0T8IEJyudR_I5rI9now@2x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно показать это и на более простом примере\n",
    "<p style=\"font-family: Arial; font-size:1em;color:red;\"> TODO </p>\n",
    "\n",
    "Перевести это в задание на самостоятельное выполнение\n",
    "\n",
    "Сделаем латентное представление признаков несвязным. И добавим к нему много бесмысленных признаков - из нормального распределения со средним 0. \n",
    "\n",
    "![alttext](img/unconnected_repr.png)\n",
    "\n",
    "\n",
    "И посмотрим, что выучил автоенкодер. Видим, что он показывает связь там, где она явно отсутствует. \n",
    "\n",
    "![alttext](img/bad_latent_unconnected.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вариационный автоэнкодер\n",
    "\n",
    "<p style=\"font-family: Arial; font-size:1em;color:red;\"> TODO </p>\n",
    "\n",
    "\n",
    "### Мотивация \n",
    "\n",
    "Хотим вместо представления слева получить представление справа\n",
    "\n",
    "![alttext](img/vae_motivation.png)\n",
    "\n",
    "При этом зоны пересечения должныы действительно содержать переходные картины \n",
    "![alttext](img/vae_ideal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u90px5yEajSf"
   },
   "source": [
    "## Попытка 1: регуляризация\n",
    "\n",
    "Можем попробовать заставить наши объекты \"лежать\" рядом - будем штрафовать латентные представления, которые далеко уходят от начала координат. \n",
    "Можем использовать как L1, так и L2 регуляризацию, так и их комбинацию - elastic loss.\n",
    "\n",
    "Однако, это приведет просто к масштабированию распределения. Нам надо одновременно получить связное латентное представление - чтобы у нас не возникало зон в латентном представлении, которым не соответствует ничего, и при этом представление, в котором цифры будут отделены друг от друга. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Значит нужно взять выходы энкодера и вызвать добавочные потери по ним. Для этого существуют функции из документации https://keras.io/api/layers/regularizers/ . Однако попытка уменьшить значения в латентном пространстве просто приведёт к его масштабированию. Нужно заставить распределение попадать в некоторые рамки. Это проиллюстрировано ниже.\n",
    "\n",
    "![alt text](https://www.jeremyjordan.me/content/images/2018/03/Screen-Shot-2018-03-18-at-7.22.24-PM.png)\n",
    "\n",
    "Мы имеем ситуацию, как на картинке слева. Переход-интерполяция между объектами проходит через зону отсутствующих в обучении объектов, декодирование которых даст несуществующие в реальности объекты. Нам не удастся погенерировать новые картинки, преобразовывая случайную точку из латентного пространства в случайную картинку. Мы хотим иметь плотное распределение, как на средней картинке (KL divergence - об этом позже), но только там классы перепутаны между собой. Значит нам нужно требовать представление в латентном пространстве размещать в виде нормального распределения, но еще и не давать разным классам перемешиваться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вероятностный автоэнкодер\n",
    "Сейчас мы сделаем математический трюк и перейдём от детерминистского энкодера и декодера, которые каждому объекту сопоставляет точку латентного пространства и наборот, к вероятностным энкодеру и декодеру. Энкодер для каждого объекта генерирует параметры нормального распределения в латентном пространстве, где можно расположить закодированный объект, а декодер использует вероятностное распределение, чтобы выбрать точку в латентном пространстве и декодировать её в исходное изображение, насколько это у него получится.\n",
    "![https://miro.medium.com/max/977/1*Q5dogodt3wzKKktE0v3dMQ@2x.png](https://miro.medium.com/max/977/1*Q5dogodt3wzKKktE0v3dMQ@2x.png)\n",
    "\n",
    "Но если просто применить такой принцип, то он снова имеет проблему предыдущего детерминистического подхода, так как вероятностное распределение сможет свернуться в дельта-функцию. Поэтому нам надо ввести регуляризацию, требующую от каждого распределения быть близким к нормальному распределению вокруг нуля координат латентного пространства с дисперсией 1. Но что такое близость двух вероятностных распределений?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дивергенция Кульбака-Лейблера\n",
    "Проблема измерения расстояния, отличия вероятностных распределений ставится в многих задачах. Например в теории информации для определения дополнительной длины сообщения для передачи информации с одним вероятностным распределением, закодированной неоптимальным кодом - сделанным для другого вероятностного распределения.\n",
    "\n",
    "Для этих задач традиционно применяют дивергенцию Кульбака-Лейблера, которая является несимметричной. Расстояние от распределения p до q не равно расстоянию от q до p. Поэтому первое распределение обычно предполагается истинным, а второе проверяемым. Проверяемое, совпадающее с истинным должно давать нулевую дивергенцию.\n",
    "\n",
    "Подробней можно посмотреть в википедии, но на самом деле нам подойдёт любая адекватная мера расстояния."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация нормальных распределений\n",
    "Итак, мы должны поменять наш энкодер, чтобы он выдавал параметры нормального распределения. Их 2 - среднее и дисперсия. Мы будем считать, что матрица ковариации диагональная. Значит нам нужно в 2 раза больше параметров. Сделаем такой энкодер, поменяв выходной слой.\n",
    "![https://miro.medium.com/max/977/1*XYyWimolMhPDMg8qCNlcwg@2x.png](https://miro.medium.com/max/977/1*XYyWimolMhPDMg8qCNlcwg@2x.png)\n",
    "\n",
    "Кстати, распределения не обязательно должны быть нормальными. Мы можем выбрать любую регуляризацию укладки объектов в латентном пространстве, хоть треугольником, хоть колечком. Все принципы останутся те же - генерация параметров этого распределения, случайная выборка точки согласно распределению вероятностей, расстояние KL между этим распределением и желаемой нами формой, чтобы распределения не схлопывались в дельта-функции (это наша регуляризация) и та же функция потерь, которая вообще и определяет задачу автоэнкодера.\n",
    "\n",
    "С декодером попроще. Он у нас не вероятностный.\n",
    "![https://miro.medium.com/max/977/1*1n6HwjwUWbmE9PvCzOVcbw@2x.png](https://miro.medium.com/max/977/1*1n6HwjwUWbmE9PvCzOVcbw@2x.png)\n",
    "\n",
    "Только как соединить параметры распределения и точку, сгенерированную по этому распределению? Удобными функциями tensorflow - никак. Придётся делать свою функцию генерации этих точек, вставлять её между энкодером и декодером, добавлять регуляризацию на основе дивергенции KL и запускать обучение. И тут второе большое разочарование - нельзя делать градиентный спуск на основе функции, генерирующей случайные точки. Это должно быть легко понять. Градиент это производная. У генератора случайных чисел с нормальными распределением какая производная? Давайте возьмём малое смещение параметров и по формуле отношения посчитаем - получается всегда плюс или минус бесконечность (много), верно? Такая система учится не будет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Приём репараметризации для сохранения возможности обратного распространения ошибки\n",
    "Мы поняли, что считать градиент от нашей функции вероятностной генерации нельзя. Поэтому мы воспользуемся свойством нормальных распределений, что распределение с параметрами среднего и дисперсии даёт такие же объекты, как сдвинутое и отмасштабированное нормальное распределение вокруг нуля и единичной дисперсией.\n",
    "![https://miro.medium.com/max/843/1*SE-A1yR7BXIjNL0tsmITZg@2x.png](https://miro.medium.com/max/843/1*SE-A1yR7BXIjNL0tsmITZg@2x.png)\n",
    "\n",
    "Тогда мы сгенерируем случайный элемент этого распределения и примем его в расчёте градиента за константу, а среднее и дисперсия будут оптимизироваться. Это уже произойдёт автоматически.\n",
    "![https://miro.medium.com/max/1395/1*S8CoO3TGtFBpzv8GvmgKeg@2x.png](https://miro.medium.com/max/1395/1*S8CoO3TGtFBpzv8GvmgKeg@2x.png)\n",
    "\n",
    "Получаем в итоге полную схему вариацинного автоэнкодера с репараметризацией и функцию потерь с регуляризацией.\n",
    "![https://miro.medium.com/max/1395/1*eRcdr8gczweQHk--1pZF9A@2x.png](https://miro.medium.com/max/1395/1*eRcdr8gczweQHk--1pZF9A@2x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте реализуем новый энкодер и декодер с ручной вставкой репараметризации и ручным циклом обучения. Начнём с 2 раза большего количества выходов у энкодера.\n",
    "\n",
    "Так как потребуется довольно много ручного кода, то будем двигаться по шагам. Сначала попробуем реализовать ручное обучение, используя только средние значения энкодера и не используя дисперсию и вообще какие-то распределения вероятностей. Для этого воспользуемся `tf.split` и возьмём половину выходов нейронов. Что будет если взять другую половину?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getDecoder and getEncoder methods are already valid\n",
    "\n",
    "VAE_latent_size = 24\n",
    "\n",
    "VAE_encoder = getEncoder(VAE_latent_size * 2)\n",
    "VAE_decoder = getDecoder(VAE_latent_size)\n",
    "\n",
    "VAE_input = Input(shape=(28, 28, 1)) # Set shape to (28, 28, 1) if autoencoder is convolutional, (784, 1) otherwise\n",
    "VAE_encoder_output = VAE_encoder(VAE_input)\n",
    "VAE_fake_generate, _ = tf.split(VAE_encoder_output, num_or_size_splits=2, axis=1)\n",
    "VAE_fake_output = VAE_decoder(VAE_fake_generate)\n",
    "fake_VAE = Model(inputs=VAE_input, outputs=VAE_fake_output)\n",
    "fake_VAE.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инструмент GradientTape\n",
    "Преобразуем нашу выборку в датасет tensorflow. Нам потребуется рассчитывать потери - введём для этого функцию, просто считающую потери по mse, как ранее для автоэнкодера, без какой-то регуляризации.\n",
    "Для ручного обучения в tensorflow есть инструмент GradientTape - лента записи событий, для которых можно просить выдать градиент по обучающимся параметрам. Он сам разбирается как такой градиент построить. Далее градиент уже передаётся оптимизатору. Тот сам разбирается как ему градиент применить для решения задачи оптимизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, y_train etc are already imported, normalized and expanded for convolutional NN\n",
    "\n",
    "epochs = 10\n",
    "train_size = len(x_train)\n",
    "batch_size = 64\n",
    "test_size = len(x_test)\n",
    "\n",
    "\n",
    "train_dataset = (tf.data.Dataset.from_tensor_slices(x_train)\n",
    "                 .shuffle(train_size).batch(batch_size))\n",
    "test_dataset = (tf.data.Dataset.from_tensor_slices(x_test)\n",
    "                .shuffle(test_size).batch(batch_size))\n",
    "\n",
    "def compute_loss(model, x):\n",
    "  mse = tf.keras.losses.MeanSquaredError()\n",
    "  return mse(model(x), x)\n",
    "\n",
    "#@tf.function\n",
    "def train_step(model, x, optimizer):\n",
    "  \"\"\"Executes one training step and returns the loss.\n",
    "\n",
    "  This function computes the loss and gradients, and uses the latter to\n",
    "  update the model's parameters.\n",
    "  \"\"\"\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss = compute_loss(model, x)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы импортируем модель расчёта времени, чтобы знать сколько длится эпоха. Эпохи это реализуемый вручную цикл вызова расчёта потерь, градиента, оптимизации. В цикле мы будем брать батч за батчем, считать время на эпоху, дополнительно для проверки обучения будем считать потери на всех батчах тестовой выборки и выводить статистику."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "  start_time = time.time()\n",
    "  for train_x in train_dataset:\n",
    "    train_step(fake_VAE, train_x, tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "  end_time = time.time()\n",
    "\n",
    "  loss = tf.keras.metrics.Mean()\n",
    "  for test_x in test_dataset:\n",
    "    loss(compute_loss(fake_VAE, test_x))\n",
    "  print('Epoch: {}, Test set loss: {}, time elapse for current epoch: {:.3}'\n",
    "        .format(epoch, loss.result(), end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь пришло время проверить, удалось ли нам вручную проводить обучение и обучить не хуже автоматических инструментов. Для этого сделаем проверку, как для автоэнкодера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_decoded = np.squeeze(fake_VAE.predict(x_test[sample_indices]), axis = 3)\n",
    "plot_images(samples_orig, \"Original x_test\")\n",
    "plot_images(samples_decoded, \"Autoencoder decoded x_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы смогли получить то же, что и раньше с автоэнкодером. Теперь нам нужно добавить вариации и репараметризацию. Меняются 2 вещи: модель и функция потерь.\n",
    "\n",
    "Меняется модель VAE, так как теперь не половина выходов энкодера это входы декодера, а все выходы это параметры для генерации точки в латентном пространстве, а уже она вход для декодера. Это сложно оформить моделью tensorflow. Для удобной передачи, как раньше в виде объекта, создадим объект VAE для обучения и использования. Этот объект позволит нам получать trainable_variables, которые нужны в обучении.\n",
    "\n",
    "Для начала покажем как энкодер передаёт сигнал декодеру на необученной сети. Для этого нам не нужна репараметризация, ведь мы не будем считать градиенты или обратное распространение ошибки.\n",
    "Цепочка расчёта у нас из 3 звеньев: энкодер, генерация точки в латентном пространстве, передача этой точки на декодер. В конце визуализация, что было на входе и что на выходе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getDecoder, getEncoder methods are already valid, and VAE_latent_size already defined\n",
    "\n",
    "# Rebuild encoder and decoder to loose connection with previous model.\n",
    "# But they are essentially the same as before.\n",
    "VAE_encoder = getEncoder(VAE_latent_size * 2)\n",
    "VAE_decoder = getDecoder(VAE_latent_size)\n",
    "\n",
    "# Get probability density parameters (interpret them as such)\n",
    "mean, logvar = tf.split(VAE_encoder(samples_orig), num_or_size_splits=2, axis=1)\n",
    "# Instantiate encoded state statistically\n",
    "z = tf.random.normal(shape=mean.shape) * tf.exp(logvar * .5) + mean\n",
    "# Run decoder with dropping one dimension for visualization\n",
    "samples_decoded = np.squeeze(VAE_decoder(z), axis = 3)\n",
    "\n",
    "plot_images(samples_orig, \"Original x_test\")\n",
    "plot_images(samples_decoded, \"Fresh VAE decoded x_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь то же самое в виде класса. Если есть способ из входа сделать выход, то назовём его recode(), а для потерь нужно только добавить MSE. Также добавим репараметризацию, как функцию, для наглядности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_larva(tf.keras.Model):\n",
    "    \"\"\"Variational autoencoder larva.\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE_larva, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = getEncoder(2 * latent_dim)\n",
    "        self.decoder = getDecoder(latent_dim)\n",
    "        self.mse = tf.keras.losses.MeanSquaredError()\n",
    "        \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "    \n",
    "    def recode(self, x):\n",
    "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        return self.decoder(z)\n",
    "        \n",
    "    def compute_loss(self, x):\n",
    "        return self.mse(x, self.recode(x))\n",
    "\n",
    "vael = VAE_larva(VAE_latent_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цикл обучения почти не поменялся. Только вызываем мы теперь методы класса, а не внешние функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, x, optimizer):\n",
    "  \"\"\"Executes one training step and returns the loss.\n",
    "\n",
    "  This function computes the loss and gradients, and uses the latter to\n",
    "  update the model's parameters.\n",
    "  \"\"\"\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss = model.compute_loss(x)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "  start_time = time.time()\n",
    "  for train_x in train_dataset:\n",
    "    train_step(vael, train_x, tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "  end_time = time.time()\n",
    "\n",
    "  loss = tf.keras.metrics.Mean()\n",
    "  for test_x in test_dataset:\n",
    "    loss(vael.compute_loss(test_x))\n",
    "  print('Epoch: {}, Test set loss: {}, time elapse for current epoch: {}'\n",
    "        .format(epoch, -loss.result(), end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы выучили вариационный автоэнкодер, но без регуляризации. Поэтому пока никаких улучшений ожидать не стоит. Не добавлена еще дивергенция KL в функцию потерь. Но снова можно проверить, что он обучился не хуже предыдущих автоэнкодеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_decoded = np.squeeze(vael.recode(samples_orig), axis = 3)\n",
    "\n",
    "plot_images(samples_orig, \"Original x_test\")\n",
    "plot_images(samples_decoded, \"VAE larva decoded x_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оптимизация дивергенции KL через нижнюю границу вариации (ELBO)\n",
    "Дивергенция KL штука сложная. Надо считать интеграл. Однако минимизация KL достигается при максимизации нижней границы вариации. Подробней об этом написано в википедии: https://en.wikipedia.org/wiki/Evidence_lower_bound Прелесть этого перехода задачи оптимизации в том, что оптимизацию параметров латентного распределения можно делать по наблюдаемым декодированным объектам, когда объекты в латентном пространстве недоступны.\n",
    "$$\\log p(x) \\ge \\text{ELBO} = \\mathbb{E}_{q(z|x)}\\left[\\log \\frac{p(x, z)}{q(z|x)}\\right].$$\n",
    "Дальше, не вдаваясь в математику, регуляторная функция будет следующая:\n",
    "$$\\log p(x|z) + \\log p(z) - \\log q(z|x),$$\n",
    "где $z$ это сэмпл из $q(z|x)$.\n",
    "Введём вспомогательную функцию `log_normal` для вычисления логарифма правдоподобия попадания элемента в нормальное распределение вероятностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(tf.keras.Model):\n",
    "    \"\"\"Variational autoencoder finally.\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = getEncoder(2 * latent_dim)\n",
    "        self.decoder = getDecoder(latent_dim)\n",
    "        self.mse = tf.keras.losses.MeanSquaredError()\n",
    "        \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "    \n",
    "    def recode(self, x):\n",
    "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        return self.decoder(z)\n",
    "        \n",
    "    def compute_loss(self, x):\n",
    "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        x_out = self.decoder(z)\n",
    "        logpx_z = -self.mse(x, x_out)\n",
    "        logpz = self.log_normal_pdf(z, 0., 0.)\n",
    "        logqz_x = self.log_normal_pdf(z, mean, logvar)\n",
    "        return -tf.reduce_mean(10*logpx_z + logpz - logqz_x)\n",
    "    \n",
    "    def log_normal_pdf(self, sample, mean, logvar, raxis=1):\n",
    "        log2pi = tf.math.log(2. * np.pi)\n",
    "        return tf.reduce_sum(\n",
    "            -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "            axis=raxis)\n",
    "\n",
    "    \n",
    "vae = VAE(VAE_latent_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По сути наша новая функция потерь это потери на некорректное восстановление изображения + потери на неточное попадание `z` в нормальное распределение с центром в нуле и единичной дисперсией - потери на неточное попадание `z` в нормальное распределение со средним и дисперсией, сгенерированных энкодером."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "  start_time = time.time()\n",
    "  for train_x in train_dataset:\n",
    "    train_step(vae, train_x, tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "  end_time = time.time()\n",
    "\n",
    "  loss = tf.keras.metrics.Mean()\n",
    "  for test_x in test_dataset:\n",
    "    loss(vae.compute_loss(test_x))\n",
    "  print('Epoch: {}, Test set ELBO: {}, time elapse for current epoch: {}'\n",
    "        .format(epoch, -loss.result(), end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, наконец, что вариационный автоэнкодер работает как автоэнкодер, визуализировав привычным способом перекодированные изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_decoded = np.squeeze(vae.recode(samples_orig), axis = 3)\n",
    "\n",
    "plot_images(samples_orig, \"Original x_test\")\n",
    "plot_images(samples_decoded, \"VAE decoded x_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы используем размерность латентного пространства 2, то это позволит нам получать распределение классов цифр на плоскости, типа такого:\n",
    "![https://www.tensorflow.org/tutorials/generative/cvae_files/output_F-ZG69QCZnGY_0.png](https://www.tensorflow.org/tutorials/generative/cvae_files/output_F-ZG69QCZnGY_0.png)\n",
    "Это не просто интерполяция по двум направлениям. Тут именно все 10 цифр должны так занять место на плоскости, чтобы плавно перетекать друг в друга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PBTLNGv4trDE"
   },
   "source": [
    "## Задание 1\n",
    "Примени автоэнкодер для освобождения картинок от шума, как мы делали в PCA, но подбери размерность латентного пространства. Какой метод фильтрации шума лучше работает: PCA или AE? Какая была размерность латентного пространства?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "colab_type": "code",
    "id": "EO07OLzPtbu2",
    "outputId": "1b572d1c-a6c4-4c65-d09f-327625062959"
   },
   "outputs": [],
   "source": [
    "noise_factor = 0.3\n",
    "\n",
    "# Add some noise to images\n",
    "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \n",
    "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) \n",
    "choice = np.random.choice(x_test.shape[0], 5)\n",
    "\n",
    "# Plot noisy and original images\n",
    "samples = x_train_noisy[choice]\n",
    "plot_images(samples.reshape(-1, 28, 28), \"Training data - noisy\")\n",
    "plot_images(x_train[choice].reshape(-1, 28, 28), \"Training data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "2ICEXaTbtbu5",
    "outputId": "04ee76e6-ee7d-484a-8a47-9ac0ab0ddce9"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "autoencoder.fit(x_train_noisy, x_train, batch_size=64, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "colab_type": "code",
    "id": "mkleDBlntbu7",
    "outputId": "51412f32-dfea-4c58-acce-d25ba477d5bb"
   },
   "outputs": [],
   "source": [
    "N_images = 6\n",
    "# Select several images from test dataset\n",
    "samples = x_test_noisy[np.random.choice(x_test_noisy.shape[0], N_images)]\n",
    "reconstructed = autoencoder.predict(samples)\n",
    "# Plot inputs and denoised outputs\n",
    "plot_images(samples.reshape(-1, 28, 28), title='Noisy images')\n",
    "plot_images(reconstructed.reshape(-1, 28, 28), title='Denoised by autoencoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2 опциональное, усложнённое\n",
    "Исправить регуляризацию в последнем VAE, чтобы он заработал. При этом использовать ссылки, визуализацию латентного пространства, интуицию, опираться на работающий без регуляризации AE. В результате сеть автоэнкодера должна смочь делать плавную интерполяцию между генерируемыми изображениями. Покажи 2 видео (или 2 набора картинок) перехода между цифрами: для автоэнкодера и для вариационного автоэнкодера (VAE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ссылки\n",
    "Более подробно про PCA и ссылка на его применение для MNIST. https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n",
    "\n",
    "Введение в вариационные автоэнкодеры через автоэнкодеры, PCA https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73\n",
    "\n",
    "Википедия по дивергенции Кульбака-Лейблера https://ru.wikipedia.org/wiki/Расстояние_Кульбака_—_Лейблера\n",
    "\n",
    "Википедия по нижней границе вариации https://en.wikipedia.org/wiki/Evidence_lower_bound\n",
    "\n",
    "Туториал по VAE от Google по tensorflow https://www.tensorflow.org/tutorials/generative/cvae"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Autoencoders lesson.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
